{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/rdma/vast-rdma/user/valmiki/bioplnn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/om2/user/valmiki/miniconda/envs/pytorch-3.10/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd /om2/user/valmiki/bioplnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import wandb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "import torchvision\n",
    "from torchvision.datasets import CIFAR10, MNIST\n",
    "from torchsparsegradutils import sparse_mm\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Feb  1 16:41:12 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 515.86.01    Driver Version: 515.86.01    CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A100 80G...  On   | 00000000:83:00.0 Off |                    0 |\n",
      "| N/A   42C    P0    76W / 300W |    489MiB / 81920MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A   1167144      C   ...s/pytorch-3.10/bin/python      487MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttrDict(dict):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.__dict__ = self\n",
    "\n",
    "\n",
    "def print_mem_stats():\n",
    "    f, t = torch.cuda.mem_get_info()\n",
    "    print(f\"Free/Total: {f/(1024**3):.2f}GB/{t/(1024**3):.2f}GB\")\n",
    "\n",
    "\n",
    "def get_dataloaders(config):\n",
    "    # Load the MNIST dataset\n",
    "    mnist_train = MNIST(\n",
    "        root=\"./data\",\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=torchvision.transforms.ToTensor(),\n",
    "    )\n",
    "\n",
    "    # Load the MNIST test dataset\n",
    "    mnist_test = MNIST(\n",
    "        root=\"./data\",\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=torchvision.transforms.ToTensor(),\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        dataset=mnist_train,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=True,\n",
    "        pin_memory=torch.cuda.is_available(),\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        dataset=mnist_test,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=False,\n",
    "        pin_memory=torch.cuda.is_available(),\n",
    "    )\n",
    "\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AttrDict(\n",
    "    # Model parameters\n",
    "    num_neurons=10000,\n",
    "    synapses_per_neuron=100,\n",
    "    sheet_size=(100, 100),\n",
    "    connectivity_std=10,\n",
    "    num_timesteps=100,\n",
    "    sheet_bias=True,\n",
    "    sheet_mm_function=torch.sparse.mm,\n",
    "    sheet_batch_first=False,\n",
    "    model_dir=\"models\",\n",
    "    # Training parameters\n",
    "    batch_size=16,\n",
    "    optimizer=optim.SGD,\n",
    "    lr=1e-3,\n",
    "    criterion=nn.CrossEntropyLoss,\n",
    "    log_freq=100,\n",
    "    num_epochs=30,\n",
    "    log_wandb=False,\n",
    ")\n",
    "try:\n",
    "    os.mkdir(config.model_dir)  # type: ignore\n",
    "except FileExistsError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorticalSheet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_neurons,\n",
    "        synapses_per_neuron,\n",
    "        bias=True,\n",
    "        mm_function=sparse_mm,\n",
    "        batch_first=False,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # Save the sparse matrix multiplication function\n",
    "        self.mm_function = mm_function\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "        # Create a sparse tensor for the weight matrix\n",
    "        indices = []\n",
    "\n",
    "        # Create adjacency matrix with normal distribution randomized weights\n",
    "        for i in range(num_neurons):\n",
    "            synapses = torch.randint(0, num_neurons, (synapses_per_neuron,))\n",
    "            synapse_root = torch.full_like(synapses, i)\n",
    "            indices.append(torch.stack((synapses, synapse_root)))\n",
    "        indices = torch.cat(indices, dim=1)\n",
    "        # Xavier initialization of values (synapses_per_neuron is the fan-in/out)\n",
    "        values = torch.randn(num_neurons * synapses_per_neuron) * math.sqrt(\n",
    "            1 / synapses_per_neuron\n",
    "        )\n",
    "\n",
    "        coo_matrix = torch.sparse_coo_tensor(\n",
    "            indices, values, (num_neurons, num_neurons), check_invariants=True\n",
    "        ).coalesce()\n",
    "        self.weight = nn.Parameter(coo_matrix)\n",
    "        self.weight.register_hook(lambda grad: print(grad))\n",
    "        # csr_matrix = coo_matrix.coalesce().to_sparse_csr()\n",
    "        # self.weight = nn.Parameter(csr_matrix)\n",
    "\n",
    "        # Initialize the bias vector\n",
    "        self.bias = nn.Parameter(torch.zeros(num_neurons, 1)) if bias else None\n",
    "\n",
    "    def coalesce(self):\n",
    "        self.weight.data = self.weight.data.coalesce()\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert self.weight.is_coalesced()\n",
    "        # x: Dense (strided) tensor of shape (batch_size, num_neurons) if\n",
    "        # batch_first, otherwise (num_neurons, batch_size)\n",
    "\n",
    "        # Transpose input if batch_first\n",
    "        if self.batch_first:\n",
    "            x = x.t()\n",
    "            \n",
    "        # Perform sparse matrix multiplication with or without bias\n",
    "        x = (\n",
    "            self.mm_function(self.weight, x)\n",
    "            if self.bias is None\n",
    "            else self.mm_function(self.weight, x)\n",
    "        )\n",
    "\n",
    "        # Transpose output back to batch first\n",
    "        if self.batch_first:\n",
    "            x = x.t()\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class CorticalRNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_neurons,\n",
    "        synapses_per_neuron,\n",
    "        num_timesteps,\n",
    "        activation=nn.GELU,\n",
    "        sheet_bias=True,\n",
    "        sheet_mm_function=torch.sparse.mm,\n",
    "        sheet_batch_first=False,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_neurons = num_neurons\n",
    "        self.num_timesteps = num_timesteps\n",
    "        self.activation = activation()\n",
    "        self.sheet_batch_first = sheet_batch_first\n",
    "\n",
    "        # Create the CorticalSheet layer\n",
    "        self.cortical_sheet = CorticalSheet(\n",
    "            num_neurons,\n",
    "            synapses_per_neuron,\n",
    "            sheet_bias,\n",
    "            sheet_mm_function,\n",
    "            sheet_batch_first,\n",
    "        )\n",
    "\n",
    "        # Create output block\n",
    "        self.out_block = nn.Sequential(\n",
    "            nn.Linear(28 * 28, 64), activation(), nn.Linear(64, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: Dense (strided) tensor of shape (batch_size, 1, 32, 32)\n",
    "\n",
    "        # Coallesce weight matrix\n",
    "        self.cortical_sheet.coalesce()\n",
    "\n",
    "        # Flatten spatial and channel dimensions\n",
    "        x = x.flatten(1)\n",
    "        # Pad with zeros for rest of neurons\n",
    "        x = F.pad(x, (0, self.num_neurons - x.shape[1]))\n",
    "\n",
    "        # To avoid tranposing x before and after every iteration, we tranpose\n",
    "        # before and after ALL iterations and do not tranpose within forward()\n",
    "        # of self.cortical_sheet\n",
    "        if not self.sheet_batch_first:\n",
    "            x = x.t()\n",
    "\n",
    "        # Pass the input through the CorticalSheet layer num_timesteps times\n",
    "        for _ in range(self.num_timesteps):\n",
    "            x = self.activation(self.cortical_sheet(x))\n",
    "\n",
    "        # Transpose back\n",
    "        if not self.sheet_batch_first:\n",
    "            x = x.t()\n",
    "\n",
    "        # Extract output from last 28*28 neurons (can be arbitrarily large number of neurons)\n",
    "        x = x[:, -28 * 28 :]\n",
    "\n",
    "        # Return classification from out_block\n",
    "        return self.out_block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopographicalCorticalSheet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        sheet_size,\n",
    "        connectivity_std,\n",
    "        synapses_per_neuron,\n",
    "        bias=True,\n",
    "        mm_function=sparse_mm,\n",
    "        batch_first=False,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # Save the sparse matrix multiplication function\n",
    "        self.sheet_size = sheet_size\n",
    "        num_neurons = sheet_size[0] * sheet_size[1]\n",
    "        self.mm_function = mm_function\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "        # Create a sparse tensor for the weight matrix\n",
    "        indices = []\n",
    "\n",
    "        # Create adjacency matrix with normal distribution randomized weights\n",
    "        for i in range(sheet_size[0]):\n",
    "            for j in range(sheet_size[1]):\n",
    "                synapses = (\n",
    "                    torch.randn(2, synapses_per_neuron)\n",
    "                    * torch.tensor((connectivity_std, connectivity_std))[\n",
    "                        :, None\n",
    "                    ]\n",
    "                    + torch.tensor((i, j))[:, None]\n",
    "                ).long()\n",
    "                synapses = synapses.clamp(\n",
    "                    torch.tensor((0, 0))[:, None],\n",
    "                    torch.tensor((sheet_size[0] - 1, sheet_size[1] - 1))[\n",
    "                        :, None\n",
    "                    ],\n",
    "                )\n",
    "                synapses = self.idx_2D_to_1D(synapses)\n",
    "                synapse_root = torch.full_like(\n",
    "                    synapses, self.idx_2D_to_1D(torch.tensor((i, j)))\n",
    "                )\n",
    "                indices.append(torch.stack((synapses, synapse_root)))\n",
    "        indices = torch.cat(indices, dim=1)\n",
    "        # Sort indices by synapses\n",
    "        # indices = indices[:, torch.argsort(indices[0])]\n",
    "        # Xavier initialization of values (synapses_per_neuron is the fan-in/out)\n",
    "        values = torch.randn(indices.shape[1]) * math.sqrt(\n",
    "            1 / synapses_per_neuron\n",
    "        )\n",
    "\n",
    "        coo_matrix = torch.sparse_coo_tensor(\n",
    "            indices, values, (num_neurons, num_neurons), check_invariants=True\n",
    "        ).coalesce()\n",
    "        csr_matrix = coo_matrix.to_sparse_csr()\n",
    "        self.weight = nn.Parameter(csr_matrix)\n",
    "        # self.weight.register_hook(lambda grad: grad.coalesce())\n",
    "        # self.weight.register_hook(lambda grad: print(grad))\n",
    "        # self.weight = nn.Parameter(csr_matrix)\n",
    "\n",
    "        # Initialize the bias vector\n",
    "        self.bias = nn.Parameter(torch.zeros(num_neurons, 1)) if bias else None\n",
    "\n",
    "    def coalesce(self):\n",
    "        self.weight.data = self.weight.data.coalesce()\n",
    "\n",
    "    def idx_1D_to_2D(self, x):\n",
    "        return torch.stack((x // self.sheet_size[1], x % self.sheet_size[1]))\n",
    "\n",
    "    def idx_2D_to_1D(self, x):\n",
    "        return x[0] * self.sheet_size[1] + x[1]\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: Dense (strided) tensor of shape (batch_size, num_neurons) if\n",
    "        # batch_first, otherwise (num_neurons, batch_size)\n",
    "        # assert self.weight.is_coalesced()\n",
    "\n",
    "        # Transpose input if batch_first\n",
    "        if self.batch_first:\n",
    "            x = x.t()\n",
    "\n",
    "        # Perform sparse matrix multiplication with or without bias\n",
    "        x = (\n",
    "            self.mm_function(self.weight, x)\n",
    "            if self.bias is None\n",
    "            else self.mm_function(self.weight, x)\n",
    "        )\n",
    "\n",
    "        # Transpose output back to batch first\n",
    "        if self.batch_first:\n",
    "            x = x.t()\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class TopographicalCorticalRNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        sheet_size,\n",
    "        connectivity_std,\n",
    "        synapses_per_neuron,\n",
    "        num_timesteps,\n",
    "        activation=nn.GELU,\n",
    "        sheet_bias=True,\n",
    "        sheet_mm_function=sparse_mm,\n",
    "        sheet_batch_first=False,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_neurons = sheet_size[0] * sheet_size[1]\n",
    "        self.num_timesteps = num_timesteps\n",
    "        self.activation = activation()\n",
    "        self.sheet_batch_first = sheet_batch_first\n",
    "\n",
    "        # Create the CorticalSheet layer\n",
    "        self.cortical_sheet = TopographicalCorticalSheet(\n",
    "            sheet_size,\n",
    "            connectivity_std,\n",
    "            synapses_per_neuron,\n",
    "            sheet_bias,\n",
    "            sheet_mm_function,\n",
    "            sheet_batch_first,\n",
    "        )\n",
    "\n",
    "        # Create output block\n",
    "        self.out_block = nn.Sequential(\n",
    "            nn.Linear(28 * 28, 64), activation(), nn.Linear(64, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: Dense (strided) tensor of shape (batch_size, 1, 32, 32)\n",
    "\n",
    "        # Coallesce weight matrix\n",
    "        # self.cortical_sheet.coalesce()\n",
    "\n",
    "        # Flatten spatial and channel dimensions\n",
    "        x = x.flatten(1)\n",
    "        # Pad with zeros for rest of neurons\n",
    "        x = F.pad(x, (0, self.num_neurons - x.shape[1]))\n",
    "\n",
    "        # To avoid tranposing x before and after every iteration, we tranpose\n",
    "        # before and after ALL iterations and do not tranpose within forward()\n",
    "        # of self.cortical_sheet\n",
    "        if not self.sheet_batch_first:\n",
    "            x = x.t()\n",
    "\n",
    "        # Pass the input through the CorticalSheet layer num_timesteps times\n",
    "        for _ in range(self.num_timesteps):\n",
    "            x = self.activation(self.cortical_sheet(x))\n",
    "\n",
    "        # Transpose back\n",
    "        if not self.sheet_batch_first:\n",
    "            x = x.t()\n",
    "\n",
    "        # Extract output from last 28*28 neurons (can be arbitrarily large number of neurons)\n",
    "        x = x[:, -28 * 28 :]\n",
    "\n",
    "        # Return classification from out_block\n",
    "        return self.out_block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = TopographicalCorticalRNN(**config).to(device)  # type: ignore\n",
    "    optimizer = config.optimizer(model.parameters(), lr=config.lr)\n",
    "    criterion = config.criterion()\n",
    "    train_loader, test_loader = get_dataloaders(config)\n",
    "\n",
    "    if config.log_wandb:\n",
    "        wandb.init(project=\"Cortical RNN\", config=config)\n",
    "\n",
    "    for epoch in range(config.num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        running_loss = 0.0\n",
    "        running_correct = 0\n",
    "        running_total = 0\n",
    "\n",
    "        bar = tqdm(\n",
    "            train_loader,\n",
    "            desc=(\n",
    "                f\"Training | Epoch: {epoch} | \"\n",
    "                f\"Loss: {0:.4f} | \"\n",
    "                f\"Acc: {0:.2%}\"\n",
    "            ),\n",
    "        )\n",
    "        for i, (images, labels) in enumerate(bar):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update statistics\n",
    "            train_loss += loss.item()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            predicted = outputs.argmax(-1)\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            train_correct += correct\n",
    "            running_correct += correct\n",
    "            train_total += len(labels)\n",
    "            running_total += len(labels)\n",
    "\n",
    "            # Log statistics\n",
    "            if (i + 1) % config.log_freq == 0:\n",
    "                running_loss /= config.log_freq\n",
    "                running_acc = running_correct / running_total\n",
    "                if config.log_wandb:\n",
    "                    wandb.log(\n",
    "                        dict(\n",
    "                            running_loss=running_loss, running_acc=running_acc\n",
    "                        )\n",
    "                    )\n",
    "                bar.set_description(\n",
    "                    f\"Training | Epoch: {epoch} | \"\n",
    "                    f\"Loss: {running_loss:.4f} | \"\n",
    "                    f\"Acc: {running_acc:.2%}\"\n",
    "                )\n",
    "                running_loss = 0\n",
    "                running_correct = 0\n",
    "                running_total = 0\n",
    "\n",
    "        # Calculate average training loss and accuracy\n",
    "        train_loss /= len(train_loader)\n",
    "        train_acc = train_correct / train_total\n",
    "\n",
    "        if config.log_wandb:\n",
    "            wandb.log(dict(train_loss=train_loss, train_acc=train_acc))\n",
    "\n",
    "        # Evaluate the model on the test set\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        test_correct = 0\n",
    "        test_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # Update statistics\n",
    "                test_loss += loss.item()\n",
    "                predicted = outputs.argmax(-1)\n",
    "                correct = (predicted == labels).sum().item()\n",
    "                test_correct += correct\n",
    "                test_total += len(labels)\n",
    "\n",
    "        # Calculate average test loss and accuracy\n",
    "        test_loss /= len(train_loader)\n",
    "        test_acc = test_correct / test_total\n",
    "\n",
    "        if config.log_wandb:\n",
    "            wandb.log(\n",
    "                dict(test_loss=test_loss, test_acc=test_acc, epoch=epoch)\n",
    "            )\n",
    "\n",
    "        # Print the epoch statistics\n",
    "        print(\n",
    "            f\"Epoch [{epoch}/{config.num_epochs}] | \"\n",
    "            f\"Train Loss: {train_loss:.4f} | \"\n",
    "            f\"Train Accuracy: {train_acc:.2%} | \"\n",
    "            f\"Test Loss: {test_loss:.4f}, \"\n",
    "            f\"Test Accuracy: {test_acc:.2%}\"\n",
    "        )\n",
    "\n",
    "        # Save Model\n",
    "        # Save Model\n",
    "        file_path = os.path.abspath(\n",
    "            os.path.join(config.model_dir, f\"model_{epoch}.pt\")\n",
    "        )\n",
    "        link_path = os.path.abspath(os.path.join(config.model_dir, \"model.pt\"))\n",
    "        torch.save(model, file_path)\n",
    "        try:\n",
    "            os.remove(link_path)\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "        os.symlink(file_path, link_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free/Total: 78.30GB/79.21GB\n"
     ]
    }
   ],
   "source": [
    "print_mem_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free/Total: 78.30GB/79.21GB\n",
      "Free/Total: 78.66GB/79.21GB\n"
     ]
    }
   ],
   "source": [
    "print_mem_stats()\n",
    "torch.cuda.empty_cache()\n",
    "print_mem_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training | Epoch: 0 | Loss: 0.0000 | Acc: 0.00%:   0%|          | 0/3750 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Could not run 'aten::_foreach_add_.List' with arguments from the 'SparseCsrCUDA' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::_foreach_add_.List' is only available for these backends: [CPU, CUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:31034 [kernel]\nCUDA: registered at aten/src/ATen/RegisterCUDA.cpp:43986 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:491 [backend fallback]\nFunctionalize: registered at aten/src/ATen/RegisterFunctionalization_2.cpp:21384 [kernel]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:63 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:15276 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:15276 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:15276 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:15276 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:15276 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:15276 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:15276 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:15276 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:15276 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:15276 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:15276 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:15276 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:15276 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:15276 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:15276 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:15276 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:15276 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:16728 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:487 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:354 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:815 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1073 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:210 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:152 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:487 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[25], line 39\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     37\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     38\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 39\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Update statistics\u001b[39;00m\n\u001b[1;32m     42\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/om2/user/valmiki/miniconda/envs/pytorch-3.10/lib/python3.10/site-packages/torch/optim/optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m                                \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 280\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    283\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m/om2/user/valmiki/miniconda/envs/pytorch-3.10/lib/python3.10/site-packages/torch/optim/optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 33\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m/om2/user/valmiki/miniconda/envs/pytorch-3.10/lib/python3.10/site-packages/torch/optim/sgd.py:76\u001b[0m, in \u001b[0;36mSGD.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     72\u001b[0m momentum_buffer_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     74\u001b[0m has_sparse_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(group, params_with_grad, d_p_list, momentum_buffer_list)\n\u001b[0;32m---> 76\u001b[0m \u001b[43msgd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43md_p_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmomentum_buffer_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmomentum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdampening\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdampening\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnesterov\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnesterov\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_sparse_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_sparse_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# update momentum_buffers in state\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p, momentum_buffer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(params_with_grad, momentum_buffer_list):\n",
      "File \u001b[0;32m/om2/user/valmiki/miniconda/envs/pytorch-3.10/lib/python3.10/site-packages/torch/optim/sgd.py:222\u001b[0m, in \u001b[0;36msgd\u001b[0;34m(params, d_p_list, momentum_buffer_list, has_sparse_grad, foreach, weight_decay, momentum, lr, dampening, nesterov, maximize)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    220\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_sgd\n\u001b[0;32m--> 222\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m     \u001b[49m\u001b[43md_p_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmomentum_buffer_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdampening\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdampening\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m     \u001b[49m\u001b[43mnesterov\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnesterov\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m     \u001b[49m\u001b[43mhas_sparse_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_sparse_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/om2/user/valmiki/miniconda/envs/pytorch-3.10/lib/python3.10/site-packages/torch/optim/sgd.py:325\u001b[0m, in \u001b[0;36m_multi_tensor_sgd\u001b[0;34m(params, grads, momentum_buffer_list, weight_decay, momentum, lr, dampening, nesterov, maximize, has_sparse_grad)\u001b[0m\n\u001b[1;32m    322\u001b[0m         device_grads \u001b[38;5;241m=\u001b[39m bufs\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m device_has_sparse_grad:\n\u001b[0;32m--> 325\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_foreach_add_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_grads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    327\u001b[0m     \u001b[38;5;66;03m# foreach APIs don't support sparse\u001b[39;00m\n\u001b[1;32m    328\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(device_params)):\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Could not run 'aten::_foreach_add_.List' with arguments from the 'SparseCsrCUDA' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::_foreach_add_.List' is only available for these backends: [CPU, CUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:31034 [kernel]\nCUDA: registered at aten/src/ATen/RegisterCUDA.cpp:43986 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:491 [backend fallback]\nFunctionalize: registered at aten/src/ATen/RegisterFunctionalization_2.cpp:21384 [kernel]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:63 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:15276 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:15276 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:15276 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:15276 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:15276 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:15276 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:15276 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:15276 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:15276 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:15276 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:15276 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:15276 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:15276 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:15276 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:15276 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:15276 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:15276 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:16728 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:487 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:354 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:815 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1073 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:210 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:152 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:487 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\n"
     ]
    }
   ],
   "source": [
    "train(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=True) as prof:\n",
    "#     train(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "autograd::engine::evaluate_function: SparseAddmmBack...         0.41%      30.035ms        62.90%        4.593s       4.176ms       0.000us         0.00%        4.674s       4.249ms          1100  \n",
      "                                              aten::add         0.43%      31.321ms        12.65%     923.936ms     422.081us       0.000us         0.00%        4.071s       1.860ms          2189  \n",
      "                                        aten::_coalesce         7.66%     559.709ms        74.94%        5.473s       1.765ms        2.738s        47.54%        3.778s       1.218ms          3101  \n",
      "                                         aten::coalesce         2.24%     163.622ms        75.06%        5.481s       1.661ms       0.000us         0.00%        3.643s       1.104ms          3301  \n",
      "                                   SparseAddmmBackward0        -0.25%  -18472.000us        59.81%        4.368s       3.971ms       0.000us         0.00%        2.944s       2.676ms          1100  \n",
      "                                            aten::addmm         0.12%       8.537ms        36.14%        2.640s       1.194ms     270.206ms         4.69%        2.455s       1.110ms          2211  \n",
      "                                    aten::_sparse_addmm         0.31%      22.644ms        36.25%        2.647s       1.209ms       0.000us         0.00%        2.445s       1.117ms          2189  \n",
      "                                              aten::cat         0.93%      68.192ms         2.37%     172.948ms      14.162us        1.744s        30.28%        1.749s     143.221us         12212  \n",
      "                                               aten::mm         0.33%      24.343ms        20.85%        1.523s     681.942us     289.475ms         5.03%        1.604s     718.177us          2233  \n",
      "void at::native::(anonymous namespace)::CatArrayBatc...         0.00%       0.000us         0.00%       0.000us       0.000us        1.311s        22.75%        1.311s       1.191ms          1100  \n",
      "void at::native::apply::coalesceValuesKernel<float, ...         0.00%       0.000us         0.00%       0.000us       0.000us     953.649ms        16.56%     953.649ms     307.629us          3100  \n",
      "void thrust::cuda_cub::core::_kernel_agent<thrust::c...         0.00%       0.000us         0.00%       0.000us       0.000us     853.120ms        14.81%     853.120ms      30.494us         27977  \n",
      "                                             aten::add_         0.51%      37.051ms         9.65%     704.766ms      95.821us      72.322ms         1.26%     732.701ms      99.619us          7355  \n",
      "                                Optimizer.step#SGD.step         0.03%       2.324ms         8.71%     636.253ms      57.841ms       0.000us         0.00%     643.967ms      58.542ms            11  \n",
      "void thrust::cuda_cub::core::_kernel_agent<thrust::c...         0.00%       0.000us         0.00%       0.000us       0.000us     476.414ms         8.27%     476.414ms     153.682us          3100  \n",
      "void at::native::(anonymous namespace)::CatArrayBatc...         0.00%       0.000us         0.00%       0.000us       0.000us     433.758ms         7.53%     433.758ms     394.325us          1100  \n",
      "                                            aten::copy_         3.20%     233.620ms        36.36%        2.656s     101.532us     292.804ms         5.08%     396.264ms      15.150us         26156  \n",
      "                                              aten::mul         1.30%      94.817ms         5.24%     383.003ms      59.723us      98.787ms         1.71%     360.604ms      56.230us          6413  \n",
      "                                       cudaLaunchKernel         8.18%     597.649ms         8.18%     597.649ms       4.771us     287.856ms         5.00%     343.034ms       2.739us        125263  \n",
      "                                 ampere_sgemm_128x64_tn         0.00%       0.000us         0.00%       0.000us       0.000us     283.597ms         4.92%     283.597ms     263.077us          1078  \n",
      "                                    cudaPeekAtLastError         0.02%       1.481ms         0.02%       1.481ms       0.010us     262.498ms         4.56%     262.498ms       1.837us        142901  \n",
      "void thrust::cuda_cub::core::_kernel_agent<thrust::c...         0.00%       0.000us         0.00%       0.000us       0.000us     246.487ms         4.28%     246.487ms       8.810us         27977  \n",
      "void cusparse::load_balancing_kernel<256u, 1u, 0ul, ...         0.00%       0.000us         0.00%       0.000us       0.000us     233.363ms         4.05%     233.363ms     107.689us          2167  \n",
      "                         Memcpy DtoD (Device -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us     227.261ms         3.95%     227.261ms      17.816us         12756  \n",
      "void thrust::cuda_cub::core::_kernel_agent<thrust::c...         0.00%       0.000us         0.00%       0.000us       0.000us     146.580ms         2.54%     146.580ms      47.284us          3100  \n",
      "                                      aten::sparse_mask         0.22%      16.067ms         3.11%     226.838ms     206.216us       0.000us         0.00%     130.269ms     118.426us          1100  \n",
      "                                                aten::t         0.15%      10.652ms         2.65%     193.533ms      82.601us       0.000us         0.00%     119.961ms      51.200us          2343  \n",
      "                                        aten::transpose         0.37%      27.209ms         2.64%     192.705ms      28.672us       0.000us         0.00%     119.961ms      17.849us          6721  \n",
      "                                            aten::clone         0.42%      30.755ms         2.38%     173.984ms      39.731us       0.000us         0.00%      97.493ms      22.264us          4379  \n",
      "                                        cudaMemcpyAsync         3.84%     280.354ms         3.84%     280.354ms      14.742us      85.842ms         1.49%      85.842ms       4.514us         19018  \n",
      "                                             aten::div_         0.92%      67.153ms         1.37%     100.352ms      16.110us      69.373ms         1.20%      83.979ms      13.482us          6229  \n",
      "void at::native::elementwise_kernel<128, 2, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us      82.858ms         1.44%      82.858ms      26.728us          3100  \n",
      "                                            aten::index         0.29%      21.151ms         0.50%      36.551ms      33.228us      79.334ms         1.38%      82.312ms      74.829us          1100  \n",
      "                                              aten::sum         0.88%      64.495ms         1.27%      92.934ms      21.949us      72.935ms         1.27%      81.399ms      19.225us          4234  \n",
      "void at::native::index_elementwise_kernel<128, 4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      79.334ms         1.38%      79.334ms      72.122us          1100  \n",
      "                                          aten::divide_         0.23%      16.954ms         1.49%     108.872ms      17.560us       0.000us         0.00%      74.976ms      12.093us          6200  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      69.373ms         1.20%      69.373ms      11.189us          6200  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us      66.179ms         1.15%      66.179ms      21.273us          3111  \n",
      "                                         aten::_to_copy         0.88%      63.986ms        33.57%        2.451s     432.054us       0.000us         0.00%      65.644ms      11.569us          5674  \n",
      "                                               aten::to         1.28%      93.477ms        33.71%        2.462s     239.605us       0.000us         0.00%      62.725ms       6.104us         10276  \n",
      "                                 cudaDeviceGetAttribute         0.01%       1.044ms         0.01%       1.044ms       0.031us      54.685ms         0.95%      54.685ms       1.604us         34100  \n",
      "void thrust::cuda_cub::core::_kernel_agent<thrust::c...         0.00%       0.000us         0.00%       0.000us       0.000us      49.140ms         0.85%      49.140ms       7.926us          6200  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      39.828ms         0.69%      39.828ms      12.848us          3100  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us      32.738ms         0.57%      32.738ms       7.478us          4378  \n",
      "                                  cudaStreamSynchronize        52.79%        3.855s        52.79%        3.855s     206.594us      28.062ms         0.49%      30.128ms       1.614us         18662  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us      28.807ms         0.50%      28.807ms       9.293us          3100  \n",
      "void at::native::elementwise_kernel<128, 2, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us      27.464ms         0.48%      27.464ms       4.993us          5500  \n",
      "                                            aten::fill_         0.33%      24.159ms         0.68%      49.720ms       3.447us      22.000ms         0.38%      26.270ms       1.821us         14423  \n",
      "                                            aten::zero_         0.11%       8.353ms         0.57%      41.939ms      12.705us       0.000us         0.00%      22.579ms       6.840us          3301  \n",
      "void convert_CooToCsr_kernel<0>(int const*, int, int...         0.00%       0.000us         0.00%       0.000us       0.000us      15.323ms         0.27%      15.323ms       7.000us          2189  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 7.303s\n",
      "Self CUDA time total: 5.760s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                        aten::_coalesce       -10.96%  -9162002.000us        96.56%       80.749s      26.090ms       70.803s        87.27%       75.788s      24.487ms          3095  \n",
      "                                         aten::coalesce         2.88%        2.406s        96.57%       80.758s      24.509ms       0.000us         0.00%       73.554s      22.323ms          3295  \n",
      "autograd::engine::evaluate_function: SparseAddmmBack...         0.03%      23.145ms        66.66%       55.751s      50.683ms       0.000us         0.00%       55.158s      50.144ms          1100  \n",
      "                                   SparseAddmmBackward0        -1.00%  -837948.000us        66.37%       55.502s      50.456ms       0.000us         0.00%       53.619s      48.744ms          1100  \n",
      "                                            aten::addmm        -1.71%  -1430865.000us        63.72%       53.286s      24.100ms     242.900ms         0.30%       50.326s      22.762ms          2211  \n",
      "                                    aten::_sparse_addmm         0.54%     450.407ms        63.73%       53.296s      24.347ms       0.000us         0.00%       49.928s      22.809ms          2189  \n",
      "void thrust::cuda_cub::core::_kernel_agent<thrust::c...         0.00%       0.000us         0.00%       0.000us       0.000us       43.396s        53.49%       43.396s       1.132ms         38351  \n",
      "                                               aten::mm        -0.47%  -391222.000us        33.20%       27.767s      12.435ms     290.174ms         0.36%       25.227s      11.297ms          2233  \n",
      "void thrust::cuda_cub::core::_kernel_agent<thrust::c...         0.00%       0.000us         0.00%       0.000us       0.000us       13.894s        17.12%       13.894s       4.491ms          3094  \n",
      "void at::native::apply::coalesceValuesKernel<float, ...         0.00%       0.000us         0.00%       0.000us       0.000us        7.337s         9.04%        7.337s       2.371ms          3094  \n",
      "                                              aten::add         0.15%     126.096ms         1.24%        1.036s      32.180us       0.000us         0.00%        3.628s     112.698us         32189  \n",
      "                                                aten::t         0.01%      11.772ms         0.27%     224.322ms      95.741us       0.000us         0.00%        3.054s       1.304ms          2343  \n",
      "                                        aten::transpose         0.01%       7.697ms         0.27%     224.742ms      33.439us       0.000us         0.00%        3.053s     454.308us          6721  \n",
      "                                              aten::mul         0.29%     239.597ms         0.78%     649.332ms      17.835us        2.746s         3.38%        2.931s      80.515us         36407  \n",
      "void thrust::cuda_cub::core::_kernel_agent<thrust::c...         0.00%       0.000us         0.00%       0.000us       0.000us        2.880s         3.55%        2.880s     930.755us          3094  \n",
      "                                            aten::copy_        12.45%       10.409s        13.78%       11.522s     205.250us        2.871s         3.54%        2.875s      51.210us         56138  \n",
      "                         Memcpy DtoD (Device -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us        2.811s         3.46%        2.811s     220.571us         12744  \n",
      "void at::native::elementwise_kernel<128, 2, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us        2.732s         3.37%        2.732s     881.467us          3099  \n",
      "                                              aten::sum         0.10%      87.576ms         0.13%     111.902ms      26.448us        2.033s         2.51%        2.033s     480.496us          4231  \n",
      "void at::native::reduce_kernel<256, 2, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us        2.026s         2.50%        2.026s     653.790us          3099  \n",
      "void thrust::cuda_cub::core::_kernel_agent<thrust::c...         0.00%       0.000us         0.00%       0.000us       0.000us        2.023s         2.49%        2.023s      52.762us         38351  \n",
      "                                              aten::cat         0.14%     113.863ms         0.29%     243.746ms      19.960us        1.573s         1.94%        1.573s     128.776us         12212  \n",
      "void thrust::cuda_cub::core::_kernel_agent<thrust::c...         0.00%       0.000us         0.00%       0.000us       0.000us        1.257s         1.55%        1.257s     203.091us          6188  \n",
      "                                            aten::clone         0.02%      16.246ms         0.23%     193.971ms      44.296us       0.000us         0.00%        1.243s     283.864us          4379  \n",
      "void at::native::(anonymous namespace)::CatArrayBatc...         0.00%       0.000us         0.00%       0.000us       0.000us        1.175s         1.45%        1.175s       1.069ms          1100  \n",
      "                                         aten::_to_copy        -0.31%  -257050.000us        13.67%       11.429s     320.416us       0.000us         0.00%        1.149s      32.216us         35668  \n",
      "                                               aten::to         0.07%      54.880ms        13.72%       11.475s     127.114us       0.000us         0.00%        1.135s      12.578us         90270  \n",
      "                                             aten::add_         0.08%      68.606ms         0.76%     634.924ms      86.467us      69.012ms         0.09%     638.329ms      86.930us          7343  \n",
      "                                Optimizer.step#SGD.step         0.00%       2.517ms         0.67%     557.601ms      50.691ms       0.000us         0.00%     569.502ms      51.773ms            11  \n",
      "void at::native::(anonymous namespace)::CatArrayBatc...         0.00%       0.000us         0.00%       0.000us       0.000us     397.162ms         0.49%     397.162ms     361.056us          1100  \n",
      "                                            aten::fill_         0.07%      61.010ms         0.08%      67.797ms       4.701us     374.601ms         0.46%     374.601ms      25.972us         14423  \n",
      "                                            aten::zero_         0.01%       7.021ms         0.06%      48.847ms      14.798us       0.000us         0.00%     371.207ms     112.453us          3301  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     364.568ms         0.45%     364.568ms     334.773us          1089  \n",
      "                                       aten::zeros_like         0.00%       3.478ms         0.04%      29.460ms      27.052us       0.000us         0.00%     334.429ms     307.097us          1089  \n",
      "                                 ampere_sgemm_128x64_tn         0.00%       0.000us         0.00%       0.000us       0.000us     284.292ms         0.35%     284.292ms     263.722us          1078  \n",
      "                                  cudaStreamSynchronize        94.17%       78.751s        94.18%       78.760s       4.229ms       0.000us         0.00%     212.140ms      11.389us         18626  \n",
      "void cusparse::load_balancing_kernel<256u, 1u, 0ul, ...         0.00%       0.000us         0.00%       0.000us       0.000us     208.112ms         0.26%     208.112ms      96.037us          2167  \n",
      "                                        cudaMemcpyAsync         0.38%     320.970ms         0.41%     339.156ms      17.856us       0.000us         0.00%     128.596ms       6.770us         18994  \n",
      "                                      aten::sparse_mask         0.01%      11.754ms         0.31%     260.102ms     236.456us       0.000us         0.00%      95.810ms      87.100us          1100  \n",
      "                                             aten::div_         0.13%     105.547ms         0.16%     134.672ms      18.365us      63.568ms         0.08%      74.517ms      10.162us          7333  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      63.568ms         0.08%      63.568ms      10.273us          6188  \n",
      "                                          aten::divide_         0.02%      13.867ms         0.16%     130.346ms      21.064us       0.000us         0.00%      58.299ms       9.421us          6188  \n",
      "                                            aten::index         0.04%      34.776ms         0.05%      42.709ms      38.826us      56.994ms         0.07%      56.994ms      51.813us          1100  \n",
      "void at::native::index_elementwise_kernel<128, 4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      56.994ms         0.07%      56.994ms      51.813us          1100  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      37.414ms         0.05%      37.414ms      12.092us          3094  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us      27.974ms         0.03%      27.974ms       6.390us          4378  \n",
      "void at::native::vectorized_elementwise_kernel<2, at...         0.00%       0.000us         0.00%       0.000us       0.000us      27.866ms         0.03%      27.866ms       9.006us          3094  \n",
      "void at::native::elementwise_kernel<128, 2, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us      27.444ms         0.03%      27.444ms       4.990us          5500  \n",
      "void convert_CooToCsr_kernel<0>(int const*, int, int...         0.00%       0.000us         0.00%       0.000us       0.000us      15.323ms         0.02%      15.323ms       7.000us          2189  \n",
      "void cusparse::binary_search_partition_kernel<256, 2...         0.00%       0.000us         0.00%       0.000us       0.000us      13.191ms         0.02%      13.191ms       6.026us          2189  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 83.630s\n",
      "Self CUDA time total: 81.134s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "indices = torch.tensor([[0, 1, 2, 3], [0, 1, 2, 3]]).long()\n",
    "values = torch.tensor([1, 2, 3, 4]).float()\n",
    "weight = torch.sparse_coo_tensor(\n",
    "    indices, values, (100000, 100000), check_invariants=True\n",
    ").coalesce()\n",
    "weight = weight.to_sparse_csr()\n",
    "weight = weight.to(device)\n",
    "weight.requires_grad = True\n",
    "B = weight.clone()\n",
    "\n",
    "weight + B\n",
    "(weight + B).sum().backward()\n",
    "print(weight.grad)\n",
    "\n",
    "\n",
    "indices = torch.tensor([[0, 1, 2, 3], [0, 1, 2, 3]]).long()\n",
    "values = torch.tensor([1, 2, 3, 4]).float()\n",
    "weight = torch.sparse_coo_tensor(\n",
    "    indices, values, (100000, 100000), check_invariants=True\n",
    ").coalesce()\n",
    "weight = weight.to(device)\n",
    "# weight = weight.to_sparse_csr()\n",
    "weight.requires_grad = True\n",
    "\n",
    "x = torch.ones(16, 1, 28, 28).to(device)\n",
    "x = x.flatten(1)\n",
    "x = F.pad(x, (0, 100000 - x.shape[1]))\n",
    "out = x.t()\n",
    "for _ in range(100):\n",
    "    out = F.relu(sparse_mm(weight, out))\n",
    "loss = out.sum()\n",
    "loss.backward()\n",
    "weight.grad\n",
    "model = CorticalRNN(**config)\n",
    "optimizer = config.optimizer(model.parameters(), lr=config.lr)\n",
    "train_loader, _ = get_dataloaders(config)\n",
    "train_iter = iter(train_loader)\n",
    "\n",
    "for _ in range(10):\n",
    "    x = next(train_iter)[0]\n",
    "    optimizer.zero_grad()\n",
    "    out = model(x)\n",
    "    loss = out.sum()\n",
    "    loss.backward()\n",
    "    print(model.cortical_sheet.weight.grad._nnz())\n",
    "    optimizer.step()\n",
    "    print(model.cortical_sheet.weight.grad._nnz())\n",
    "model.cortical_sheet.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CorticalRNN\n",
      "cortical_sheet.weight 995140\n",
      "cortical_sheet.bias 10000\n",
      "out_block.0.weight 50176\n",
      "out_block.0.bias 64\n",
      "out_block.2.weight 640\n",
      "out_block.2.bias 10\n",
      "Total Parameters: 1056030\n",
      "\n",
      "TopographicalCorticalRNN\n",
      "cortical_sheet.weight 907791\n",
      "cortical_sheet.bias 10000\n",
      "out_block.0.weight 50176\n",
      "out_block.0.bias 64\n",
      "out_block.2.weight 640\n",
      "out_block.2.bias 10\n",
      "Total Parameters: 968681\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "print(\"CorticalRNN\")\n",
    "model = CorticalRNN(**config)\n",
    "total_params = 0\n",
    "for param in model.named_parameters():\n",
    "    num_params = (\n",
    "        param[1]._nnz()\n",
    "        if param[0] == \"cortical_sheet.weight\"\n",
    "        else param[1].numel()\n",
    "    )\n",
    "    total_params += num_params\n",
    "    print(param[0], num_params)\n",
    "print(f\"Total Parameters: {total_params}\\n\")\n",
    "\n",
    "print(\"TopographicalCorticalRNN\")\n",
    "model = TopographicalCorticalRNN(**config)\n",
    "total_params = 0\n",
    "for param in model.named_parameters():\n",
    "    num_params = (\n",
    "        param[1]._nnz()\n",
    "        if param[0] == \"cortical_sheet.weight\"\n",
    "        else param[1].numel()\n",
    "    )\n",
    "    total_params += num_params\n",
    "    print(param[0], num_params)\n",
    "print(f\"Total Parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
