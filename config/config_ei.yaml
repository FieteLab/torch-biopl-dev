# Data and Model parameters
model:
  input_size: [28, 28]
  input_dim: 1
  h_pyr_dim: 4
  h_inter_dims: [4, 4, 4, 4]
  fb_dim: 4
  exc_kernel_size: [5, 5]
  inh_kernel_size: [5, 5]
  num_layers: 3
  num_steps: 20
  num_classes: 10
  fb_adjacency: 
  - [0, 0, 0]
  - [1, 0, 0]
  - [1, 1, 0]
  pool_kernel_size: [5, 5]
  pool_stride: [2, 2]
  bias: True
  activation: relu
  fc_dim: 256
# Training parameters
data:
  dataset: mnist
  root: data
  batch_size: 16
  num_workers: 0
optimizer: 
  fn: adam
  lr: 0.001
  momentum: 0.9
  beta1: 0.9
  beta2: 0.999
criterion: cross_entropy
wandb: True
compile:
  disable: True
  fullgraph: True
  dynamic: null
  backend: inductor
  mode: "max-autotune"
train:
  epochs: 30
  log_freq: 5
  model_dir: models
  grad_clip:
    disable: True
    type: norm
    value: 1.0