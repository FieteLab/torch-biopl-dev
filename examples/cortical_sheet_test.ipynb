{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/rdma/vast-rdma/user/valmiki/bioplnn\n"
     ]
    }
   ],
   "source": [
    "%cd /om2/user/valmiki/bioplnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import wandb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "import torchvision\n",
    "from torchvision.datasets import CIFAR10, MNIST\n",
    "import torchsparsegradutils as tsgu\n",
    "import torch_sparse\n",
    "from tqdm import tqdm\n",
    "from bioplnn.topography import TopographicalRNN\n",
    "from bioplnn.utils import AttrDict, print_mem_stats\n",
    "from bioplnn.dataset import get_MNIST_V1_dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Feb 26 17:34:25 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 515.86.01    Driver Version: 515.86.01    CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A100 80G...  On   | 00000000:43:00.0 Off |                    0 |\n",
      "| N/A   49C    P0    62W / 300W |      0MiB / 81920MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AttrDict(\n",
    "    # Model parameters\n",
    "    # num_neurons=100000,\n",
    "    data_dir=\"data\",\n",
    "    retina_path=\"connection/V1_indices.npy\",\n",
    "    sheet_size=None,\n",
    "    connectivity_std=10,\n",
    "    synapses_per_neuron=32,\n",
    "    num_timesteps=100,\n",
    "    sheet_bias=True,\n",
    "    sheet_mm_function=\"torch_sparse\",\n",
    "    sheet_sparse_format=\"torch_sparse\",\n",
    "    sheet_batch_first=False,\n",
    "    adjacency_matrix_path=\"connection/sparse.pt\",\n",
    "    input_indices=\"connection/V1_indices_flat.pt\",\n",
    "    output_indices=\"connection/V4_indices_flat.pt\",\n",
    "    activation=\"relu\",\n",
    "    model_dir=\"models\",\n",
    "    # Training parameters\n",
    "    batch_size=16,\n",
    "    optimizer=optim.SGD,\n",
    "    lr=1e-3,\n",
    "    criterion=nn.CrossEntropyLoss,\n",
    "    log_freq=50,\n",
    "    num_epochs=30,\n",
    "    wandb=True,\n",
    "    num_workers=0,\n",
    ")\n",
    "try:\n",
    "    os.mkdir(config.model_dir)  # type: ignore\n",
    "except FileExistsError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = TopographicalRNN(**config).to(device)  # type: ignore\n",
    "    optimizer = config.optimizer(model.parameters(), lr=config.lr)\n",
    "    criterion = config.criterion()\n",
    "    train_loader, test_loader = get_MNIST_V1_dataloaders(\n",
    "        root=config.data_dir,\n",
    "        retina_path=config.retina_path,\n",
    "        batch_size=config.batch_size,\n",
    "        num_workers=config.num_workers,\n",
    "    )\n",
    "\n",
    "    if config.wandb:\n",
    "        wandb.init(project=\"Cortical RNN\", config=config)\n",
    "\n",
    "    for epoch in range(config.num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        running_loss = 0.0\n",
    "        running_correct = 0\n",
    "        running_total = 0\n",
    "\n",
    "        bar = tqdm(\n",
    "            train_loader,\n",
    "            desc=(\n",
    "                f\"Training | Epoch: {epoch} | \"\n",
    "                f\"Loss: {0:.4f} | \"\n",
    "                f\"Acc: {0:.2%}\"\n",
    "            ),\n",
    "        )\n",
    "        for i, (images, labels) in enumerate(bar):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update statistics\n",
    "            train_loss += loss.item()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            predicted = outputs.argmax(-1)\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            train_correct += correct\n",
    "            running_correct += correct\n",
    "            train_total += len(labels)\n",
    "            running_total += len(labels)\n",
    "\n",
    "            # Log statistics\n",
    "            if (i + 1) % config.log_freq == 0:\n",
    "                running_loss /= config.log_freq\n",
    "                running_acc = running_correct / running_total\n",
    "                if config.wandb:\n",
    "                    wandb.log(\n",
    "                        dict(\n",
    "                            running_loss=running_loss, running_acc=running_acc\n",
    "                        )\n",
    "                    )\n",
    "                bar.set_description(\n",
    "                    f\"Training | Epoch: {epoch} | \"\n",
    "                    f\"Loss: {running_loss:.4f} | \"\n",
    "                    f\"Acc: {running_acc:.2%}\"\n",
    "                )\n",
    "                running_loss = 0\n",
    "                running_correct = 0\n",
    "                running_total = 0\n",
    "\n",
    "        # Calculate average training loss and accuracy\n",
    "        train_loss /= len(train_loader)\n",
    "        train_acc = train_correct / train_total\n",
    "\n",
    "        if config.wandb:\n",
    "            wandb.log(dict(train_loss=train_loss, train_acc=train_acc))\n",
    "\n",
    "        # Evaluate the model on the test set\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        test_correct = 0\n",
    "        test_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # Update statistics\n",
    "                test_loss += loss.item()\n",
    "                predicted = outputs.argmax(-1)\n",
    "                correct = (predicted == labels).sum().item()\n",
    "                test_correct += correct\n",
    "                test_total += len(labels)\n",
    "\n",
    "        # Calculate average test loss and accuracy\n",
    "        test_loss /= len(train_loader)\n",
    "        test_acc = test_correct / test_total\n",
    "\n",
    "        if config.wandb:\n",
    "            wandb.log(\n",
    "                dict(test_loss=test_loss, test_acc=test_acc, epoch=epoch)\n",
    "            )\n",
    "\n",
    "        # Print the epoch statistics\n",
    "        print(\n",
    "            f\"Epoch [{epoch}/{config.num_epochs}] | \"\n",
    "            f\"Train Loss: {train_loss:.4f} | \"\n",
    "            f\"Train Accuracy: {train_acc:.2%} | \"\n",
    "            f\"Test Loss: {test_loss:.4f}, \"\n",
    "            f\"Test Accuracy: {test_acc:.2%}\"\n",
    "        )\n",
    "\n",
    "        # Save Model\n",
    "        # Save Model\n",
    "        file_path = os.path.abspath(\n",
    "            os.path.join(config.model_dir, f\"model_{epoch}.pt\")\n",
    "        )\n",
    "        link_path = os.path.abspath(os.path.join(config.model_dir, \"model.pt\"))\n",
    "        torch.save(model, file_path)\n",
    "        try:\n",
    "            os.remove(link_path)\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "        os.symlink(file_path, link_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/rdma/vast-rdma/user/valmiki/bioplnn/src/bioplnn/topography.py:218: UserWarning: If adjacency_matrix_path is provided, sheet_size, connectivity_std, and synapses_per_neuron will be ignored\n",
      "  warn(\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvalmiki-kothare-vk\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/rdma/vast-rdma/user/valmiki/bioplnn/wandb/run-20240226_173429-ecreltyw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/valmiki-kothare-vk/Cortical%20RNN/runs/ecreltyw' target=\"_blank\">peachy-fog-13</a></strong> to <a href='https://wandb.ai/valmiki-kothare-vk/Cortical%20RNN' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/valmiki-kothare-vk/Cortical%20RNN' target=\"_blank\">https://wandb.ai/valmiki-kothare-vk/Cortical%20RNN</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/valmiki-kothare-vk/Cortical%20RNN/runs/ecreltyw' target=\"_blank\">https://wandb.ai/valmiki-kothare-vk/Cortical%20RNN/runs/ecreltyw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training | Epoch: 0 | Loss: 2.3090 | Acc: 9.38%:  14%|█▍        | 528/3750 [03:01<1:18:29,  1.46s/it]"
     ]
    }
   ],
   "source": [
    "train(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=True) as prof:\n",
    "#     train(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "autograd::engine::evaluate_function: SparseAddmmBack...         0.41%      30.035ms        62.90%        4.593s       4.176ms       0.000us         0.00%        4.674s       4.249ms          1100  \n",
      "                                              aten::add         0.43%      31.321ms        12.65%     923.936ms     422.081us       0.000us         0.00%        4.071s       1.860ms          2189  \n",
      "                                        aten::_coalesce         7.66%     559.709ms        74.94%        5.473s       1.765ms        2.738s        47.54%        3.778s       1.218ms          3101  \n",
      "                                         aten::coalesce         2.24%     163.622ms        75.06%        5.481s       1.661ms       0.000us         0.00%        3.643s       1.104ms          3301  \n",
      "                                   SparseAddmmBackward0        -0.25%  -18472.000us        59.81%        4.368s       3.971ms       0.000us         0.00%        2.944s       2.676ms          1100  \n",
      "                                            aten::addmm         0.12%       8.537ms        36.14%        2.640s       1.194ms     270.206ms         4.69%        2.455s       1.110ms          2211  \n",
      "                                    aten::_sparse_addmm         0.31%      22.644ms        36.25%        2.647s       1.209ms       0.000us         0.00%        2.445s       1.117ms          2189  \n",
      "                                              aten::cat         0.93%      68.192ms         2.37%     172.948ms      14.162us        1.744s        30.28%        1.749s     143.221us         12212  \n",
      "                                               aten::mm         0.33%      24.343ms        20.85%        1.523s     681.942us     289.475ms         5.03%        1.604s     718.177us          2233  \n",
      "void at::native::(anonymous namespace)::CatArrayBatc...         0.00%       0.000us         0.00%       0.000us       0.000us        1.311s        22.75%        1.311s       1.191ms          1100  \n",
      "void at::native::apply::coalesceValuesKernel<float, ...         0.00%       0.000us         0.00%       0.000us       0.000us     953.649ms        16.56%     953.649ms     307.629us          3100  \n",
      "void thrust::cuda_cub::core::_kernel_agent<thrust::c...         0.00%       0.000us         0.00%       0.000us       0.000us     853.120ms        14.81%     853.120ms      30.494us         27977  \n",
      "                                             aten::add_         0.51%      37.051ms         9.65%     704.766ms      95.821us      72.322ms         1.26%     732.701ms      99.619us          7355  \n",
      "                                Optimizer.step#SGD.step         0.03%       2.324ms         8.71%     636.253ms      57.841ms       0.000us         0.00%     643.967ms      58.542ms            11  \n",
      "void thrust::cuda_cub::core::_kernel_agent<thrust::c...         0.00%       0.000us         0.00%       0.000us       0.000us     476.414ms         8.27%     476.414ms     153.682us          3100  \n",
      "void at::native::(anonymous namespace)::CatArrayBatc...         0.00%       0.000us         0.00%       0.000us       0.000us     433.758ms         7.53%     433.758ms     394.325us          1100  \n",
      "                                            aten::copy_         3.20%     233.620ms        36.36%        2.656s     101.532us     292.804ms         5.08%     396.264ms      15.150us         26156  \n",
      "                                              aten::mul         1.30%      94.817ms         5.24%     383.003ms      59.723us      98.787ms         1.71%     360.604ms      56.230us          6413  \n",
      "                                       cudaLaunchKernel         8.18%     597.649ms         8.18%     597.649ms       4.771us     287.856ms         5.00%     343.034ms       2.739us        125263  \n",
      "                                 ampere_sgemm_128x64_tn         0.00%       0.000us         0.00%       0.000us       0.000us     283.597ms         4.92%     283.597ms     263.077us          1078  \n",
      "                                    cudaPeekAtLastError         0.02%       1.481ms         0.02%       1.481ms       0.010us     262.498ms         4.56%     262.498ms       1.837us        142901  \n",
      "void thrust::cuda_cub::core::_kernel_agent<thrust::c...         0.00%       0.000us         0.00%       0.000us       0.000us     246.487ms         4.28%     246.487ms       8.810us         27977  \n",
      "void cusparse::load_balancing_kernel<256u, 1u, 0ul, ...         0.00%       0.000us         0.00%       0.000us       0.000us     233.363ms         4.05%     233.363ms     107.689us          2167  \n",
      "                         Memcpy DtoD (Device -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us     227.261ms         3.95%     227.261ms      17.816us         12756  \n",
      "void thrust::cuda_cub::core::_kernel_agent<thrust::c...         0.00%       0.000us         0.00%       0.000us       0.000us     146.580ms         2.54%     146.580ms      47.284us          3100  \n",
      "                                      aten::sparse_mask         0.22%      16.067ms         3.11%     226.838ms     206.216us       0.000us         0.00%     130.269ms     118.426us          1100  \n",
      "                                                aten::t         0.15%      10.652ms         2.65%     193.533ms      82.601us       0.000us         0.00%     119.961ms      51.200us          2343  \n",
      "                                        aten::transpose         0.37%      27.209ms         2.64%     192.705ms      28.672us       0.000us         0.00%     119.961ms      17.849us          6721  \n",
      "                                            aten::clone         0.42%      30.755ms         2.38%     173.984ms      39.731us       0.000us         0.00%      97.493ms      22.264us          4379  \n",
      "                                        cudaMemcpyAsync         3.84%     280.354ms         3.84%     280.354ms      14.742us      85.842ms         1.49%      85.842ms       4.514us         19018  \n",
      "                                             aten::div_         0.92%      67.153ms         1.37%     100.352ms      16.110us      69.373ms         1.20%      83.979ms      13.482us          6229  \n",
      "void at::native::elementwise_kernel<128, 2, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us      82.858ms         1.44%      82.858ms      26.728us          3100  \n",
      "                                            aten::index         0.29%      21.151ms         0.50%      36.551ms      33.228us      79.334ms         1.38%      82.312ms      74.829us          1100  \n",
      "                                              aten::sum         0.88%      64.495ms         1.27%      92.934ms      21.949us      72.935ms         1.27%      81.399ms      19.225us          4234  \n",
      "void at::native::index_elementwise_kernel<128, 4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      79.334ms         1.38%      79.334ms      72.122us          1100  \n",
      "                                          aten::divide_         0.23%      16.954ms         1.49%     108.872ms      17.560us       0.000us         0.00%      74.976ms      12.093us          6200  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      69.373ms         1.20%      69.373ms      11.189us          6200  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us      66.179ms         1.15%      66.179ms      21.273us          3111  \n",
      "                                         aten::_to_copy         0.88%      63.986ms        33.57%        2.451s     432.054us       0.000us         0.00%      65.644ms      11.569us          5674  \n",
      "                                               aten::to         1.28%      93.477ms        33.71%        2.462s     239.605us       0.000us         0.00%      62.725ms       6.104us         10276  \n",
      "                                 cudaDeviceGetAttribute         0.01%       1.044ms         0.01%       1.044ms       0.031us      54.685ms         0.95%      54.685ms       1.604us         34100  \n",
      "void thrust::cuda_cub::core::_kernel_agent<thrust::c...         0.00%       0.000us         0.00%       0.000us       0.000us      49.140ms         0.85%      49.140ms       7.926us          6200  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      39.828ms         0.69%      39.828ms      12.848us          3100  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us      32.738ms         0.57%      32.738ms       7.478us          4378  \n",
      "                                  cudaStreamSynchronize        52.79%        3.855s        52.79%        3.855s     206.594us      28.062ms         0.49%      30.128ms       1.614us         18662  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us      28.807ms         0.50%      28.807ms       9.293us          3100  \n",
      "void at::native::elementwise_kernel<128, 2, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us      27.464ms         0.48%      27.464ms       4.993us          5500  \n",
      "                                            aten::fill_         0.33%      24.159ms         0.68%      49.720ms       3.447us      22.000ms         0.38%      26.270ms       1.821us         14423  \n",
      "                                            aten::zero_         0.11%       8.353ms         0.57%      41.939ms      12.705us       0.000us         0.00%      22.579ms       6.840us          3301  \n",
      "void convert_CooToCsr_kernel<0>(int const*, int, int...         0.00%       0.000us         0.00%       0.000us       0.000us      15.323ms         0.27%      15.323ms       7.000us          2189  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 7.303s\n",
      "Self CUDA time total: 5.760s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                        aten::_coalesce       -10.96%  -9162002.000us        96.56%       80.749s      26.090ms       70.803s        87.27%       75.788s      24.487ms          3095  \n",
      "                                         aten::coalesce         2.88%        2.406s        96.57%       80.758s      24.509ms       0.000us         0.00%       73.554s      22.323ms          3295  \n",
      "autograd::engine::evaluate_function: SparseAddmmBack...         0.03%      23.145ms        66.66%       55.751s      50.683ms       0.000us         0.00%       55.158s      50.144ms          1100  \n",
      "                                   SparseAddmmBackward0        -1.00%  -837948.000us        66.37%       55.502s      50.456ms       0.000us         0.00%       53.619s      48.744ms          1100  \n",
      "                                            aten::addmm        -1.71%  -1430865.000us        63.72%       53.286s      24.100ms     242.900ms         0.30%       50.326s      22.762ms          2211  \n",
      "                                    aten::_sparse_addmm         0.54%     450.407ms        63.73%       53.296s      24.347ms       0.000us         0.00%       49.928s      22.809ms          2189  \n",
      "void thrust::cuda_cub::core::_kernel_agent<thrust::c...         0.00%       0.000us         0.00%       0.000us       0.000us       43.396s        53.49%       43.396s       1.132ms         38351  \n",
      "                                               aten::mm        -0.47%  -391222.000us        33.20%       27.767s      12.435ms     290.174ms         0.36%       25.227s      11.297ms          2233  \n",
      "void thrust::cuda_cub::core::_kernel_agent<thrust::c...         0.00%       0.000us         0.00%       0.000us       0.000us       13.894s        17.12%       13.894s       4.491ms          3094  \n",
      "void at::native::apply::coalesceValuesKernel<float, ...         0.00%       0.000us         0.00%       0.000us       0.000us        7.337s         9.04%        7.337s       2.371ms          3094  \n",
      "                                              aten::add         0.15%     126.096ms         1.24%        1.036s      32.180us       0.000us         0.00%        3.628s     112.698us         32189  \n",
      "                                                aten::t         0.01%      11.772ms         0.27%     224.322ms      95.741us       0.000us         0.00%        3.054s       1.304ms          2343  \n",
      "                                        aten::transpose         0.01%       7.697ms         0.27%     224.742ms      33.439us       0.000us         0.00%        3.053s     454.308us          6721  \n",
      "                                              aten::mul         0.29%     239.597ms         0.78%     649.332ms      17.835us        2.746s         3.38%        2.931s      80.515us         36407  \n",
      "void thrust::cuda_cub::core::_kernel_agent<thrust::c...         0.00%       0.000us         0.00%       0.000us       0.000us        2.880s         3.55%        2.880s     930.755us          3094  \n",
      "                                            aten::copy_        12.45%       10.409s        13.78%       11.522s     205.250us        2.871s         3.54%        2.875s      51.210us         56138  \n",
      "                         Memcpy DtoD (Device -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us        2.811s         3.46%        2.811s     220.571us         12744  \n",
      "void at::native::elementwise_kernel<128, 2, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us        2.732s         3.37%        2.732s     881.467us          3099  \n",
      "                                              aten::sum         0.10%      87.576ms         0.13%     111.902ms      26.448us        2.033s         2.51%        2.033s     480.496us          4231  \n",
      "void at::native::reduce_kernel<256, 2, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us        2.026s         2.50%        2.026s     653.790us          3099  \n",
      "void thrust::cuda_cub::core::_kernel_agent<thrust::c...         0.00%       0.000us         0.00%       0.000us       0.000us        2.023s         2.49%        2.023s      52.762us         38351  \n",
      "                                              aten::cat         0.14%     113.863ms         0.29%     243.746ms      19.960us        1.573s         1.94%        1.573s     128.776us         12212  \n",
      "void thrust::cuda_cub::core::_kernel_agent<thrust::c...         0.00%       0.000us         0.00%       0.000us       0.000us        1.257s         1.55%        1.257s     203.091us          6188  \n",
      "                                            aten::clone         0.02%      16.246ms         0.23%     193.971ms      44.296us       0.000us         0.00%        1.243s     283.864us          4379  \n",
      "void at::native::(anonymous namespace)::CatArrayBatc...         0.00%       0.000us         0.00%       0.000us       0.000us        1.175s         1.45%        1.175s       1.069ms          1100  \n",
      "                                         aten::_to_copy        -0.31%  -257050.000us        13.67%       11.429s     320.416us       0.000us         0.00%        1.149s      32.216us         35668  \n",
      "                                               aten::to         0.07%      54.880ms        13.72%       11.475s     127.114us       0.000us         0.00%        1.135s      12.578us         90270  \n",
      "                                             aten::add_         0.08%      68.606ms         0.76%     634.924ms      86.467us      69.012ms         0.09%     638.329ms      86.930us          7343  \n",
      "                                Optimizer.step#SGD.step         0.00%       2.517ms         0.67%     557.601ms      50.691ms       0.000us         0.00%     569.502ms      51.773ms            11  \n",
      "void at::native::(anonymous namespace)::CatArrayBatc...         0.00%       0.000us         0.00%       0.000us       0.000us     397.162ms         0.49%     397.162ms     361.056us          1100  \n",
      "                                            aten::fill_         0.07%      61.010ms         0.08%      67.797ms       4.701us     374.601ms         0.46%     374.601ms      25.972us         14423  \n",
      "                                            aten::zero_         0.01%       7.021ms         0.06%      48.847ms      14.798us       0.000us         0.00%     371.207ms     112.453us          3301  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     364.568ms         0.45%     364.568ms     334.773us          1089  \n",
      "                                       aten::zeros_like         0.00%       3.478ms         0.04%      29.460ms      27.052us       0.000us         0.00%     334.429ms     307.097us          1089  \n",
      "                                 ampere_sgemm_128x64_tn         0.00%       0.000us         0.00%       0.000us       0.000us     284.292ms         0.35%     284.292ms     263.722us          1078  \n",
      "                                  cudaStreamSynchronize        94.17%       78.751s        94.18%       78.760s       4.229ms       0.000us         0.00%     212.140ms      11.389us         18626  \n",
      "void cusparse::load_balancing_kernel<256u, 1u, 0ul, ...         0.00%       0.000us         0.00%       0.000us       0.000us     208.112ms         0.26%     208.112ms      96.037us          2167  \n",
      "                                        cudaMemcpyAsync         0.38%     320.970ms         0.41%     339.156ms      17.856us       0.000us         0.00%     128.596ms       6.770us         18994  \n",
      "                                      aten::sparse_mask         0.01%      11.754ms         0.31%     260.102ms     236.456us       0.000us         0.00%      95.810ms      87.100us          1100  \n",
      "                                             aten::div_         0.13%     105.547ms         0.16%     134.672ms      18.365us      63.568ms         0.08%      74.517ms      10.162us          7333  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      63.568ms         0.08%      63.568ms      10.273us          6188  \n",
      "                                          aten::divide_         0.02%      13.867ms         0.16%     130.346ms      21.064us       0.000us         0.00%      58.299ms       9.421us          6188  \n",
      "                                            aten::index         0.04%      34.776ms         0.05%      42.709ms      38.826us      56.994ms         0.07%      56.994ms      51.813us          1100  \n",
      "void at::native::index_elementwise_kernel<128, 4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      56.994ms         0.07%      56.994ms      51.813us          1100  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      37.414ms         0.05%      37.414ms      12.092us          3094  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us      27.974ms         0.03%      27.974ms       6.390us          4378  \n",
      "void at::native::vectorized_elementwise_kernel<2, at...         0.00%       0.000us         0.00%       0.000us       0.000us      27.866ms         0.03%      27.866ms       9.006us          3094  \n",
      "void at::native::elementwise_kernel<128, 2, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us      27.444ms         0.03%      27.444ms       4.990us          5500  \n",
      "void convert_CooToCsr_kernel<0>(int const*, int, int...         0.00%       0.000us         0.00%       0.000us       0.000us      15.323ms         0.02%      15.323ms       7.000us          2189  \n",
      "void cusparse::binary_search_partition_kernel<256, 2...         0.00%       0.000us         0.00%       0.000us       0.000us      13.191ms         0.02%      13.191ms       6.026us          2189  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 83.630s\n",
      "Self CUDA time total: 81.134s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "indices = torch.tensor([[0, 1, 2, 3], [0, 1, 2, 3]]).long()\n",
    "values = torch.tensor([1, 2, 3, 4]).float()\n",
    "weight = torch.sparse_coo_tensor(\n",
    "    indices, values, (100000, 100000), check_invariants=True\n",
    ").coalesce()\n",
    "weight = weight.to_sparse_csr()\n",
    "weight = weight.to(device)\n",
    "weight.requires_grad = True\n",
    "B = weight.clone()\n",
    "\n",
    "weight + B\n",
    "(weight + B).sum().backward()\n",
    "print(weight.grad)\n",
    "\n",
    "\n",
    "indices = torch.tensor([[0, 1, 2, 3], [0, 1, 2, 3]]).long()\n",
    "values = torch.tensor([1, 2, 3, 4]).float()\n",
    "weight = torch.sparse_coo_tensor(\n",
    "    indices, values, (100000, 100000), check_invariants=True\n",
    ").coalesce()\n",
    "weight = weight.to(device)\n",
    "# weight = weight.to_sparse_csr()\n",
    "weight.requires_grad = True\n",
    "\n",
    "x = torch.ones(16, 1, 28, 28).to(device)\n",
    "x = x.flatten(1)\n",
    "x = F.pad(x, (0, 100000 - x.shape[1]))\n",
    "out = x.t()\n",
    "for _ in range(100):\n",
    "    out = F.relu(sparse_mm(weight, out))\n",
    "loss = out.sum()\n",
    "loss.backward()\n",
    "weight.grad\n",
    "model = CorticalRNN(**config)\n",
    "optimizer = config.optimizer(model.parameters(), lr=config.lr)\n",
    "train_loader, _ = get_dataloaders(config)\n",
    "train_iter = iter(train_loader)\n",
    "\n",
    "for _ in range(10):\n",
    "    x = next(train_iter)[0]\n",
    "    optimizer.zero_grad()\n",
    "    out = model(x)\n",
    "    loss = out.sum()\n",
    "    loss.backward()\n",
    "    print(model.cortical_sheet.weight.grad._nnz())\n",
    "    optimizer.step()\n",
    "    print(model.cortical_sheet.weight.grad._nnz())\n",
    "model.cortical_sheet.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CorticalRNN\n",
      "cortical_sheet.weight 995140\n",
      "cortical_sheet.bias 10000\n",
      "out_block.0.weight 50176\n",
      "out_block.0.bias 64\n",
      "out_block.2.weight 640\n",
      "out_block.2.bias 10\n",
      "Total Parameters: 1056030\n",
      "\n",
      "TopographicalCorticalRNN\n",
      "cortical_sheet.weight 907791\n",
      "cortical_sheet.bias 10000\n",
      "out_block.0.weight 50176\n",
      "out_block.0.bias 64\n",
      "out_block.2.weight 640\n",
      "out_block.2.bias 10\n",
      "Total Parameters: 968681\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "print(\"CorticalRNN\")\n",
    "model = CorticalRNN(**config)\n",
    "total_params = 0\n",
    "for param in model.named_parameters():\n",
    "    num_params = (\n",
    "        param[1]._nnz()\n",
    "        if param[0] == \"cortical_sheet.weight\"\n",
    "        else param[1].numel()\n",
    "    )\n",
    "    total_params += num_params\n",
    "    print(param[0], num_params)\n",
    "print(f\"Total Parameters: {total_params}\\n\")\n",
    "\n",
    "print(\"TopographicalCorticalRNN\")\n",
    "model = TopographicalCorticalRNN(**config)\n",
    "total_params = 0\n",
    "for param in model.named_parameters():\n",
    "    num_params = (\n",
    "        param[1]._nnz()\n",
    "        if param[0] == \"cortical_sheet.weight\"\n",
    "        else param[1].numel()\n",
    "    )\n",
    "    total_params += num_params\n",
    "    print(param[0], num_params)\n",
    "print(f\"Total Parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CorticalSheet(nn.Module):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         num_neurons,\n",
    "#         synapses_per_neuron,\n",
    "#         bias=True,\n",
    "#         mm_function=sparse_mm,\n",
    "#         batch_first=False,\n",
    "#         **kwargs\n",
    "#     ):\n",
    "#         super().__init__()\n",
    "#         # Save the sparse matrix multiplication function\n",
    "#         self.mm_function = mm_function\n",
    "#         self.batch_first = batch_first\n",
    "\n",
    "#         # Create a sparse tensor for the weight matrix\n",
    "#         indices = []\n",
    "\n",
    "#         # Create adjacency matrix with normal distribution randomized weights\n",
    "#         for i in range(num_neurons):\n",
    "#             synapses = torch.randint(0, num_neurons, (synapses_per_neuron,))\n",
    "#             synapse_root = torch.full_like(synapses, i)\n",
    "#             indices.append(torch.stack((synapses, synapse_root)))\n",
    "#         indices = torch.cat(indices, dim=1)\n",
    "#         # Xavier initialization of values (synapses_per_neuron is the fan-in/out)\n",
    "#         values = torch.randn(num_neurons * synapses_per_neuron) * math.sqrt(\n",
    "#             1 / synapses_per_neuron\n",
    "#         )\n",
    "\n",
    "#         coo_matrix = torch.sparse_coo_tensor(\n",
    "#             indices, values, (num_neurons, num_neurons), check_invariants=True\n",
    "#         ).coalesce()\n",
    "#         self.weight = nn.Parameter(coo_matrix)\n",
    "#         self.weight.register_hook(lambda grad: print(grad))\n",
    "#         # csr_matrix = coo_matrix.coalesce().to_sparse_csr()\n",
    "#         # self.weight = nn.Parameter(csr_matrix)\n",
    "\n",
    "#         # Initialize the bias vector\n",
    "#         self.bias = nn.Parameter(torch.zeros(num_neurons, 1)) if bias else None\n",
    "\n",
    "#     def coalesce(self):\n",
    "#         self.weight.data = self.weight.data.coalesce()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         assert self.weight.is_coalesced()\n",
    "#         # x: Dense (strided) tensor of shape (batch_size, num_neurons) if\n",
    "#         # batch_first, otherwise (num_neurons, batch_size)\n",
    "\n",
    "#         # Transpose input if batch_first\n",
    "#         if self.batch_first:\n",
    "#             x = x.t()\n",
    "\n",
    "#         # Perform sparse matrix multiplication with or without bias\n",
    "#         x = (\n",
    "#             self.mm_function(self.weight, x)\n",
    "#             if self.bias is None\n",
    "#             else self.mm_function(self.weight, x)\n",
    "#         )\n",
    "\n",
    "#         # Transpose output back to batch first\n",
    "#         if self.batch_first:\n",
    "#             x = x.t()\n",
    "\n",
    "#         return x\n",
    "\n",
    "\n",
    "# class CorticalRNN(nn.Module):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         num_neurons,\n",
    "#         synapses_per_neuron,\n",
    "#         num_timesteps,\n",
    "#         activation=nn.GELU,\n",
    "#         sheet_bias=True,\n",
    "#         sheet_mm_function=torch.sparse.mm,\n",
    "#         sheet_batch_first=False,\n",
    "#         **kwargs\n",
    "#     ):\n",
    "#         super().__init__()\n",
    "#         self.num_neurons = num_neurons\n",
    "#         self.num_timesteps = num_timesteps\n",
    "#         self.activation = activation()\n",
    "#         self.sheet_batch_first = sheet_batch_first\n",
    "\n",
    "#         # Create the CorticalSheet layer\n",
    "#         self.cortical_sheet = CorticalSheet(\n",
    "#             num_neurons,\n",
    "#             synapses_per_neuron,\n",
    "#             sheet_bias,\n",
    "#             sheet_mm_function,\n",
    "#             sheet_batch_first,\n",
    "#         )\n",
    "\n",
    "#         # Create output block\n",
    "#         self.out_block = nn.Sequential(\n",
    "#             nn.Linear(28 * 28, 64), activation(), nn.Linear(64, 10)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # x: Dense (strided) tensor of shape (batch_size, 1, 32, 32)\n",
    "\n",
    "#         # Coallesce weight matrix\n",
    "#         self.cortical_sheet.coalesce()\n",
    "\n",
    "#         # Flatten spatial and channel dimensions\n",
    "#         x = x.flatten(1)\n",
    "#         # Pad with zeros for rest of neurons\n",
    "#         x = F.pad(x, (0, self.num_neurons - x.shape[1]))\n",
    "\n",
    "#         # To avoid tranposing x before and after every iteration, we tranpose\n",
    "#         # before and after ALL iterations and do not tranpose within forward()\n",
    "#         # of self.cortical_sheet\n",
    "#         if not self.sheet_batch_first:\n",
    "#             x = x.t()\n",
    "\n",
    "#         # Pass the input through the CorticalSheet layer num_timesteps times\n",
    "#         for _ in range(self.num_timesteps):\n",
    "#             x = self.activation(self.cortical_sheet(x))\n",
    "\n",
    "#         # Transpose back\n",
    "#         if not self.sheet_batch_first:\n",
    "#             x = x.t()\n",
    "\n",
    "#         # Extract output from last 28*28 neurons (can be arbitrarily large number of neurons)\n",
    "#         x = x[:, -28 * 28 :]\n",
    "\n",
    "#         # Return classification from out_block\n",
    "#         return self.out_block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
