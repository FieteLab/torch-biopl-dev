{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/rdma/vast-rdma/user/valmiki/bioplnn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/om2/user/valmiki/miniconda/envs/pytorch/lib/python3.11/site-packages/IPython/core/magics/osm.py:393: UserWarning: This is now an optional IPython functionality, using bookmarks requires you to install the `pickleshare` library.\n",
      "  bkms = self.shell.db.get('bookmarks', {})\n"
     ]
    }
   ],
   "source": [
    "%cd ~/om2/bioplnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from bioplnn.utils import AttrDict\n",
    "from bioplnn.topography import TopographicalRNN\n",
    "from bioplnn.dataset import get_dataloaders\n",
    "from bioplnn.sparse_sgd import SparseSGD\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dict(\n",
    "    sheet_size=[(150, 300)],\n",
    "    connectivity_std=[1, 10, 100, 1000],\n",
    "    synapses_per_neuron=[10, 100, 1000, 10000],\n",
    "    num_timesteps=[100],\n",
    "    bias=[True],\n",
    "    mm_function=[\"torch_sparse\", \"native\", \"tsgu\"],\n",
    "    sparse_format=[\"torch_sparse\", \"coo\", \"csr\"],\n",
    "    batch_first=[True],\n",
    "    adjacency_matrix_path=[None],\n",
    "    self_recurrence=[True],\n",
    "    input_indices=[\"connection/V1_indices_flat.pt\"],\n",
    "    output_indices=[\"connection/V4_indices_flat.pt\"],\n",
    "    activation=[\"relu\", \"tanh\"],\n",
    "    batch_size=[4, 16, 64, 256, 1024, 4096],\n",
    ")\n",
    "grid = ParameterGrid(config)\n",
    "\n",
    "lr = 1e-3\n",
    "momentum = 0.9\n",
    "\n",
    "start_idx = 20\n",
    "num_iters = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: {'activation': 'relu', 'adjacency_matrix_path': None, 'batch_first': True, 'batch_size': 4, 'bias': True, 'connectivity_std': 1, 'input_indices': 'connection/V1_indices_flat.pt', 'mm_function': 'torch_sparse', 'num_timesteps': 100, 'output_indices': 'connection/V4_indices_flat.pt', 'self_recurrence': True, 'sheet_size': (150, 300), 'sparse_format': 'torch_sparse', 'synapses_per_neuron': 10}\n",
      "Avg time: 0.0862 s\n",
      "--------------------------------------------------------------------------------\n",
      "Params: {'activation': 'relu', 'adjacency_matrix_path': None, 'batch_first': True, 'batch_size': 4, 'bias': True, 'connectivity_std': 1, 'input_indices': 'connection/V1_indices_flat.pt', 'mm_function': 'torch_sparse', 'num_timesteps': 100, 'output_indices': 'connection/V4_indices_flat.pt', 'self_recurrence': True, 'sheet_size': (150, 300), 'sparse_format': 'torch_sparse', 'synapses_per_neuron': 100}\n",
      "Avg time: 0.1096 s\n",
      "--------------------------------------------------------------------------------\n",
      "Params: {'activation': 'relu', 'adjacency_matrix_path': None, 'batch_first': True, 'batch_size': 4, 'bias': True, 'connectivity_std': 1, 'input_indices': 'connection/V1_indices_flat.pt', 'mm_function': 'torch_sparse', 'num_timesteps': 100, 'output_indices': 'connection/V4_indices_flat.pt', 'self_recurrence': True, 'sheet_size': (150, 300), 'sparse_format': 'torch_sparse', 'synapses_per_neuron': 1000}\n",
      "Avg time: 0.1468 s\n",
      "--------------------------------------------------------------------------------\n",
      "Params: {'activation': 'relu', 'adjacency_matrix_path': None, 'batch_first': True, 'batch_size': 4, 'bias': True, 'connectivity_std': 1, 'input_indices': 'connection/V1_indices_flat.pt', 'mm_function': 'torch_sparse', 'num_timesteps': 100, 'output_indices': 'connection/V4_indices_flat.pt', 'self_recurrence': True, 'sheet_size': (150, 300), 'sparse_format': 'torch_sparse', 'synapses_per_neuron': 10000}\n",
      "Avg time: 0.1517 s\n",
      "--------------------------------------------------------------------------------\n",
      "Params: {'activation': 'relu', 'adjacency_matrix_path': None, 'batch_first': True, 'batch_size': 4, 'bias': True, 'connectivity_std': 1, 'input_indices': 'connection/V1_indices_flat.pt', 'mm_function': 'torch_sparse', 'num_timesteps': 100, 'output_indices': 'connection/V4_indices_flat.pt', 'self_recurrence': True, 'sheet_size': (150, 300), 'sparse_format': 'coo', 'synapses_per_neuron': 10}\n",
      "sparse_format must not be specified if mm_function is 'torch_sparse'\n",
      "Params: {'activation': 'relu', 'adjacency_matrix_path': None, 'batch_first': True, 'batch_size': 4, 'bias': True, 'connectivity_std': 1, 'input_indices': 'connection/V1_indices_flat.pt', 'mm_function': 'torch_sparse', 'num_timesteps': 100, 'output_indices': 'connection/V4_indices_flat.pt', 'self_recurrence': True, 'sheet_size': (150, 300), 'sparse_format': 'coo', 'synapses_per_neuron': 100}\n",
      "sparse_format must not be specified if mm_function is 'torch_sparse'\n",
      "Params: {'activation': 'relu', 'adjacency_matrix_path': None, 'batch_first': True, 'batch_size': 4, 'bias': True, 'connectivity_std': 1, 'input_indices': 'connection/V1_indices_flat.pt', 'mm_function': 'torch_sparse', 'num_timesteps': 100, 'output_indices': 'connection/V4_indices_flat.pt', 'self_recurrence': True, 'sheet_size': (150, 300), 'sparse_format': 'coo', 'synapses_per_neuron': 1000}\n",
      "sparse_format must not be specified if mm_function is 'torch_sparse'\n",
      "Params: {'activation': 'relu', 'adjacency_matrix_path': None, 'batch_first': True, 'batch_size': 4, 'bias': True, 'connectivity_std': 1, 'input_indices': 'connection/V1_indices_flat.pt', 'mm_function': 'torch_sparse', 'num_timesteps': 100, 'output_indices': 'connection/V4_indices_flat.pt', 'self_recurrence': True, 'sheet_size': (150, 300), 'sparse_format': 'coo', 'synapses_per_neuron': 10000}\n",
      "sparse_format must not be specified if mm_function is 'torch_sparse'\n",
      "Params: {'activation': 'relu', 'adjacency_matrix_path': None, 'batch_first': True, 'batch_size': 4, 'bias': True, 'connectivity_std': 1, 'input_indices': 'connection/V1_indices_flat.pt', 'mm_function': 'torch_sparse', 'num_timesteps': 100, 'output_indices': 'connection/V4_indices_flat.pt', 'self_recurrence': True, 'sheet_size': (150, 300), 'sparse_format': 'csr', 'synapses_per_neuron': 10}\n",
      "sparse_format must not be specified if mm_function is 'torch_sparse'\n",
      "Params: {'activation': 'relu', 'adjacency_matrix_path': None, 'batch_first': True, 'batch_size': 4, 'bias': True, 'connectivity_std': 1, 'input_indices': 'connection/V1_indices_flat.pt', 'mm_function': 'torch_sparse', 'num_timesteps': 100, 'output_indices': 'connection/V4_indices_flat.pt', 'self_recurrence': True, 'sheet_size': (150, 300), 'sparse_format': 'csr', 'synapses_per_neuron': 100}\n",
      "sparse_format must not be specified if mm_function is 'torch_sparse'\n",
      "Params: {'activation': 'relu', 'adjacency_matrix_path': None, 'batch_first': True, 'batch_size': 4, 'bias': True, 'connectivity_std': 1, 'input_indices': 'connection/V1_indices_flat.pt', 'mm_function': 'torch_sparse', 'num_timesteps': 100, 'output_indices': 'connection/V4_indices_flat.pt', 'self_recurrence': True, 'sheet_size': (150, 300), 'sparse_format': 'csr', 'synapses_per_neuron': 1000}\n",
      "sparse_format must not be specified if mm_function is 'torch_sparse'\n",
      "Params: {'activation': 'relu', 'adjacency_matrix_path': None, 'batch_first': True, 'batch_size': 4, 'bias': True, 'connectivity_std': 1, 'input_indices': 'connection/V1_indices_flat.pt', 'mm_function': 'torch_sparse', 'num_timesteps': 100, 'output_indices': 'connection/V4_indices_flat.pt', 'self_recurrence': True, 'sheet_size': (150, 300), 'sparse_format': 'csr', 'synapses_per_neuron': 10000}\n",
      "sparse_format must not be specified if mm_function is 'torch_sparse'\n",
      "Params: {'activation': 'relu', 'adjacency_matrix_path': None, 'batch_first': True, 'batch_size': 4, 'bias': True, 'connectivity_std': 1, 'input_indices': 'connection/V1_indices_flat.pt', 'mm_function': 'native', 'num_timesteps': 100, 'output_indices': 'connection/V4_indices_flat.pt', 'self_recurrence': True, 'sheet_size': (150, 300), 'sparse_format': 'torch_sparse', 'synapses_per_neuron': 10}\n",
      "sparse_format must be 'coo' or 'csr' if mm_function is 'native' or 'tsgu'\n",
      "Params: {'activation': 'relu', 'adjacency_matrix_path': None, 'batch_first': True, 'batch_size': 4, 'bias': True, 'connectivity_std': 1, 'input_indices': 'connection/V1_indices_flat.pt', 'mm_function': 'native', 'num_timesteps': 100, 'output_indices': 'connection/V4_indices_flat.pt', 'self_recurrence': True, 'sheet_size': (150, 300), 'sparse_format': 'torch_sparse', 'synapses_per_neuron': 100}\n",
      "sparse_format must be 'coo' or 'csr' if mm_function is 'native' or 'tsgu'\n",
      "Params: {'activation': 'relu', 'adjacency_matrix_path': None, 'batch_first': True, 'batch_size': 4, 'bias': True, 'connectivity_std': 1, 'input_indices': 'connection/V1_indices_flat.pt', 'mm_function': 'native', 'num_timesteps': 100, 'output_indices': 'connection/V4_indices_flat.pt', 'self_recurrence': True, 'sheet_size': (150, 300), 'sparse_format': 'torch_sparse', 'synapses_per_neuron': 1000}\n",
      "sparse_format must be 'coo' or 'csr' if mm_function is 'native' or 'tsgu'\n",
      "Params: {'activation': 'relu', 'adjacency_matrix_path': None, 'batch_first': True, 'batch_size': 4, 'bias': True, 'connectivity_std': 1, 'input_indices': 'connection/V1_indices_flat.pt', 'mm_function': 'native', 'num_timesteps': 100, 'output_indices': 'connection/V4_indices_flat.pt', 'self_recurrence': True, 'sheet_size': (150, 300), 'sparse_format': 'torch_sparse', 'synapses_per_neuron': 10000}\n",
      "sparse_format must be 'coo' or 'csr' if mm_function is 'native' or 'tsgu'\n",
      "Params: {'activation': 'relu', 'adjacency_matrix_path': None, 'batch_first': True, 'batch_size': 4, 'bias': True, 'connectivity_std': 1, 'input_indices': 'connection/V1_indices_flat.pt', 'mm_function': 'native', 'num_timesteps': 100, 'output_indices': 'connection/V4_indices_flat.pt', 'self_recurrence': True, 'sheet_size': (150, 300), 'sparse_format': 'coo', 'synapses_per_neuron': 10}\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Could not run 'aten::_foreach_mul_.Scalar' with arguments from the 'SparseCUDA' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::_foreach_mul_.Scalar' is only available for these backends: [CPU, CUDA, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:31357 [kernel]\nCUDA: registered at aten/src/ATen/RegisterCUDA.cpp:44411 [kernel]\nMeta: registered at /dev/null:241 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:154 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at aten/src/ATen/RegisterFunctionalization_1.cpp:25069 [kernel]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18740 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18740 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18740 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18740 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18740 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18740 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18740 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18740 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18740 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18740 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18740 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18740 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18740 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18740 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18740 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18740 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18740 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_3.cpp:14672 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:378 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:244 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:720 [backend fallback]\nBatchedNestedTensor: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:746 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:162 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:166 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:158 [backend fallback]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 50\u001b[0m\n\u001b[1;32m     48\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     49\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 50\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAvg time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_time\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39mnum_iters\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m s\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m80\u001b[39m)\n",
      "File \u001b[0;32m/om2/user/valmiki/miniconda/envs/pytorch/lib/python3.11/site-packages/torch/optim/optimizer.py:385\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    381\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    382\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    383\u001b[0m             )\n\u001b[0;32m--> 385\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    388\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m/om2/user/valmiki/miniconda/envs/pytorch/lib/python3.11/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m/om2/user/valmiki/miniconda/envs/pytorch/lib/python3.11/site-packages/torch/optim/sgd.py:75\u001b[0m, in \u001b[0;36mSGD.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     71\u001b[0m momentum_buffer_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     73\u001b[0m has_sparse_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(group, params_with_grad, d_p_list, momentum_buffer_list)\n\u001b[0;32m---> 75\u001b[0m \u001b[43msgd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43md_p_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmomentum_buffer_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmomentum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdampening\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdampening\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnesterov\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnesterov\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_sparse_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_sparse_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# update momentum_buffers in state\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p, momentum_buffer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(params_with_grad, momentum_buffer_list):\n",
      "File \u001b[0;32m/om2/user/valmiki/miniconda/envs/pytorch/lib/python3.11/site-packages/torch/optim/sgd.py:220\u001b[0m, in \u001b[0;36msgd\u001b[0;34m(params, d_p_list, momentum_buffer_list, has_sparse_grad, foreach, weight_decay, momentum, lr, dampening, nesterov, maximize)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    218\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_sgd\n\u001b[0;32m--> 220\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m     \u001b[49m\u001b[43md_p_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmomentum_buffer_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdampening\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdampening\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m     \u001b[49m\u001b[43mnesterov\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnesterov\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m     \u001b[49m\u001b[43mhas_sparse_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_sparse_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/om2/user/valmiki/miniconda/envs/pytorch/lib/python3.11/site-packages/torch/optim/sgd.py:307\u001b[0m, in \u001b[0;36m_multi_tensor_sgd\u001b[0;34m(params, grads, momentum_buffer_list, weight_decay, momentum, lr, dampening, nesterov, maximize, has_sparse_grad)\u001b[0m\n\u001b[1;32m    304\u001b[0m         bufs\u001b[38;5;241m.\u001b[39mappend(device_momentum_buffer_list[i])\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m all_states_with_momentum_buffer:\n\u001b[0;32m--> 307\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_foreach_mul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbufs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    308\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_foreach_add_(bufs, device_grads, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m dampening)\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Could not run 'aten::_foreach_mul_.Scalar' with arguments from the 'SparseCUDA' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::_foreach_mul_.Scalar' is only available for these backends: [CPU, CUDA, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:31357 [kernel]\nCUDA: registered at aten/src/ATen/RegisterCUDA.cpp:44411 [kernel]\nMeta: registered at /dev/null:241 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:154 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at aten/src/ATen/RegisterFunctionalization_1.cpp:25069 [kernel]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18740 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18740 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18740 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18740 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18740 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18740 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18740 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18740 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18740 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18740 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18740 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18740 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18740 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18740 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18740 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18740 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18740 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_3.cpp:14672 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:378 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:244 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:720 [backend fallback]\nBatchedNestedTensor: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:746 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:162 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:166 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:158 [backend fallback]\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for params in grid:\n",
    "    print(f\"Params: {params}\")\n",
    "    params = AttrDict(params)\n",
    "    batch_size = params.batch_size\n",
    "    del params.batch_size\n",
    "    try:\n",
    "        model = TopographicalRNN(**params).to(device)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        continue\n",
    "\n",
    "    if params.mm_function == \"torch_sparse\" or params.sparse_format == \"coo\":\n",
    "        optimizer = torch.optim.SGD(\n",
    "            model.parameters(),\n",
    "            lr=lr,\n",
    "            momentum=momentum,\n",
    "        )\n",
    "    else:\n",
    "        optimizer = SparseSGD(\n",
    "            model.parameters(),\n",
    "            lr=lr,\n",
    "            momentum=momentum,\n",
    "        )\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    train_loader, test_loader = get_dataloaders(\n",
    "        dataset=\"mnist_v1\",\n",
    "        root=\"data\",\n",
    "        retina_path=\"connection/V1_indices.npy\",\n",
    "        batch_size=batch_size,\n",
    "        num_workers=0,\n",
    "    )\n",
    "\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        if i == start_idx:\n",
    "            start_time = time.time()\n",
    "        elif i == start_idx + num_iters:\n",
    "            total_time = time.time() - start_time\n",
    "            break\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Avg time: {total_time / num_iters:.4f} s\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
