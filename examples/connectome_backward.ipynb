{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing gradients and backpropagating through connectome-initialized models\n",
    "\n",
    "While the previous tutorial showed you how to initialize a neural network with connectome-determined weights, it didn't provide you with information on how to \"tune\" model parameters in a data-driven manner. Turns out, computing (and backpropagating) gradients in networks with both ***sparse and dense*** tensors is non-trivial. In this tutorial, we spin up a small example on how you can accomplish this in `torch-biopl`. \n",
    "\n",
    "Goals:\n",
    "\n",
    "- Continue from our previous example.\n",
    "- Implement a wrapper on top of the `ConnectomeODERNN` to support a readout layer.\n",
    "- Setup a simple training loop.\n",
    "\n",
    "Note: For demonstration purposes, we'll use flattened MNIST images as inputs into the connectome. This is however simplistic and we do allow for arbitrarily complex input mappings. To learn how to do that please refer to the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "from tqdm import tqdm\n",
    "\n",
    "from bioplnn.models import ConnectomeODEClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "print(\"Using device: {}\".format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the connectome and read it in as a torch tensor. We have pre-processed this as a sparse tensor for the purposes of this example.\n",
    "save_dir = \"connectivity/turaga\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "# !gdown \"https://drive.google.com/uc?id=18448HYpYrm60boziHG73bxN4CK5jG-1g\" -O \"{save_dir}/turaga-dros-visual-connectome.pt\"\n",
    "connectome = torch.load(\n",
    "    os.path.join(save_dir, \"turaga-dros-visual-connectome.pt\"),\n",
    "    weights_only=True,\n",
    ")\n",
    "\n",
    "from bioplnn.utils import create_sparse_projection\n",
    "\n",
    "# since we are feeding in MNIST images\n",
    "input_size = 28 * 28\n",
    "num_neurons = connectome.shape[0]\n",
    "\n",
    "input_projection_matrix = create_sparse_projection(\n",
    "    size=input_size,\n",
    "    num_neurons=num_neurons,\n",
    "    indices=torch.randint(high=num_neurons, size=(input_size,)),\n",
    "    mode=\"ih\",\n",
    ")\n",
    "\n",
    "# for now, lets just read outputs from all neurons\n",
    "output_projection_matrix = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the classifier wrapper\n",
    "\n",
    "Here, we have written a simple utility that adds an output projecting layer from the connectome to the desired logit space. Again, this is simply an example. Please feel free to add sophistication to this as you please."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConnectomeODEClassifier(\n",
    "    rnn_kwargs={\n",
    "        \"input_size\": input_size,\n",
    "        \"num_neurons\": num_neurons,\n",
    "        \"connectome\": torch.abs(connectome),\n",
    "        \"input_projection\": input_projection_matrix,\n",
    "        \"output_projection\": output_projection_matrix,\n",
    "        \"neuron_nonlinearity\": \"Sigmoid\",\n",
    "        \"batch_first\": False,\n",
    "        \"compile_solver_kwargs\": {\n",
    "            \"mode\": \"max-autotune\",\n",
    "            \"dynamic\": False,\n",
    "            \"fullgraph\": True,\n",
    "        },\n",
    "    },\n",
    "    num_classes=10,\n",
    "    fc_dim=256,\n",
    "    dropout=0.5,\n",
    ").to(device)\n",
    "print(model)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader setup\n",
    "transform = T.Compose([T.ToTensor(), T.Normalize((0.1307,), (0.3081,))])\n",
    "train_data = MNIST(root=\"data\", train=True, transform=transform, download=True)\n",
    "train_loader = DataLoader(\n",
    "    train_data, batch_size=128, num_workers=0, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 1\n",
    "# print training statistics every five batches\n",
    "log_frequency = 50\n",
    "model = model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note: Things to look out for\n",
    "\n",
    "- Depending on the GPU you have access to and the number of CPUs you will have to adjust `batch_size` and `num_workers` in the DataLoader, as well as `num_steps` in the model forward pass. For reference, on a A100 GPU and a single CPU `batch_size = 256`, `num_workers = 0`, and `num_steps=5` is a reasonable estimate.\n",
    "- In this example, we have used the torch compiler with the `max-autotune` flag. This means the first few steps in the first epoch WILL BE EXTREMELY SLOW. But, this will dramatically improve as the training goes on. Fret not, early on!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "running_loss, running_correct, running_total = 0, 0, 0\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i, (x, labels) in enumerate(train_loader):\n",
    "        x = x.to(device)\n",
    "        labels = labels.to(device)\n",
    "        torch._inductor.cudagraph_mark_step_begin()\n",
    "        logits = model(x, num_steps=2)\n",
    "        loss = criterion(logits, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate running accuracy and loss\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        running_total += labels.size(0)\n",
    "        running_correct += (predicted == labels).sum().item()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        running_acc = running_correct / running_total\n",
    "        if (i + 1) % log_frequency == 0:\n",
    "            print(\n",
    "                f\"Training | Epoch: {epoch} | \"\n",
    "                + f\"Loss: {running_loss:.4f} | \"\n",
    "                + f\"Acc: {running_acc:.2%}\"\n",
    "            )\n",
    "            running_loss, running_correct, running_total = 0, 0, 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bioplnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
